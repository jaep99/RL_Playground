{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Validation of **Torque Intervention Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Date: 2024-09-01 ~ 2024-09-??\n",
    "- Authors: GT SRL - Unicycle Team\n",
    "  - Hyeonjae (Jae) Park\n",
    "  - Myungsun (Paul) Park\n",
    "  - Yunho Cho\n",
    "  - Shreyas Kousik (PI)\n",
    "- Notes:\n",
    "  - [Link to Google Slides for editing figures](https://docs.google.com/presentation/d/1wJeh8KGyOOuEBuaoZyRqSNGG6kMHLQ6Taz8T7vFEMv8/edit#slide=id.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to empirically test whether it is feasible to **accelerate the training of a ML-/RL-based controller** via *torque intervention*. Here, we define \"torque intervention\" as the injection of torque into the system by a simultaneously participating agent (e.g., \"coach agent\"). The final action will be the action of the \"student agent\", modified by the action of the \"coach agent\". \n",
    "\n",
    "Notes on why we came up with this idea:\n",
    "- We hypothesize that it is possible to *simplify the behavior of the \"environment\" (i.e., the dynamical system)* so that an agent perceives it as a simpler system. \n",
    "- Thus, it will be easier for the agent to successfully achieve the control objective (e.g., balancing, moving, turning...). \n",
    "  - This is (probably?) analogous to the concept of [feedback linearization](https://en.wikipedia.org/wiki/Feedback_linearization) in classical control. \n",
    "  - A real-world example: Modern drivers do not know—and do not **need** to know—the working mechanisms of the automatic gearbox or anti-lock braking system (ABS) in order to successfully operate the car, given that the corresponding control systems are reliable. \n",
    "- Fundamentally, this is the abstraction of complexity. \n",
    "  - Which is ubiquitous and fundamental in our lives. \n",
    "  - Yet, we don't know whether ML/RL algorithms can feasibly generate policies that effectively abstract the dynamical complexity (e.g., nonlinearities) of a system... in the eyes of another RL agent.  \n",
    "\n",
    "Here, a (relatively) comprehensive definition of the torque intervention process, along with unknowns (independent variables) is as follows: \n",
    "- The student **may or may not** be informed of the actions of the coach.\n",
    "  - On one hand, we risk \"overtaking\" the student's agency.\n",
    "    - For example: the student can learn just to relay the coach's actions. \n",
    "      - An open question is whether this can be avoided via reward design (so that it *can* exploit the coach, but it incentivized not to, and thus doesn't). \n",
    "  - On the other hand, we risk confusing the student with varying environment behavior (dynamics) that is not communicated. \n",
    "- The coach **probably** needs to know what the optimal control action is. \n",
    "  - Case 1. The coach is initialized from an \"expert\" policy, which knows how to control the system.\n",
    "  - Case 2. The coach is informed by a separate \"expert\" policy, \". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
